# Enhanced Neon PostgreSQL Configuration and Optimization
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-optimization-config
  namespace: flextime-production
  labels:
    app: postgres-optimization
data:
  postgresql.conf: |
    # Connection Settings
    max_connections = 200
    shared_preload_libraries = 'pg_stat_statements,auto_explain,pg_cron'
    
    # Memory Configuration
    shared_buffers = 256MB
    effective_cache_size = 1GB
    maintenance_work_mem = 64MB
    checkpoint_completion_target = 0.9
    wal_buffers = 16MB
    default_statistics_target = 100
    random_page_cost = 1.1
    effective_io_concurrency = 200
    
    # Checkpoint and WAL Configuration
    max_wal_size = 1GB
    min_wal_size = 80MB
    checkpoint_timeout = 5min
    checkpoint_completion_target = 0.9
    archive_mode = on
    archive_command = '/usr/local/bin/archive_wal.sh %p %f'
    
    # Query Optimization
    enable_partitionwise_join = on
    enable_partitionwise_aggregate = on
    enable_parallel_append = on
    enable_parallel_hash = on
    max_parallel_workers_per_gather = 2
    max_parallel_workers = 8
    max_parallel_maintenance_workers = 2
    
    # Logging Configuration
    log_destination = 'stderr'
    logging_collector = on
    log_directory = 'pg_log'
    log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
    log_rotation_age = 1d
    log_rotation_size = 10MB
    log_min_duration_statement = 1000
    log_checkpoints = on
    log_connections = on
    log_disconnections = on
    log_lock_waits = on
    log_statement = 'mod'
    log_temp_files = 0
    
    # Performance Monitoring
    track_activities = on
    track_counts = on
    track_io_timing = on
    track_functions = all
    
    # Auto Explain
    auto_explain.log_min_duration = 1000
    auto_explain.log_analyze = on
    auto_explain.log_buffers = on
    auto_explain.log_timing = on
    auto_explain.log_triggers = on
    auto_explain.log_verbose = on
    auto_explain.log_nested_statements = on
    
    # pg_stat_statements
    pg_stat_statements.max = 10000
    pg_stat_statements.track = all
    pg_stat_statements.save = on
    
  backup_config.sh: |
    #!/bin/bash
    
    # Backup Configuration for FlexTime Database
    export PGUSER="${POSTGRES_USER}"
    export PGPASSWORD="${POSTGRES_PASSWORD}"
    export PGHOST="${POSTGRES_HOST}"
    export PGPORT="${POSTGRES_PORT:-5432}"
    export PGDATABASE="${POSTGRES_DB}"
    
    # S3 Configuration
    export AWS_REGION="${AWS_REGION:-us-west-2}"
    export S3_BUCKET="${S3_BACKUP_BUCKET}"
    export BACKUP_RETENTION_DAYS="${BACKUP_RETENTION_DAYS:-30}"
    
    # Backup Settings
    BACKUP_DIR="/tmp/backups"
    TIMESTAMP=$(date +%Y%m%d_%H%M%S)
    BACKUP_FILE="flextime_backup_${TIMESTAMP}.sql"
    BACKUP_LOG="/var/log/postgres/backup_${TIMESTAMP}.log"
    
    # Create backup directory
    mkdir -p ${BACKUP_DIR}
    mkdir -p /var/log/postgres
    
    # Function to log messages
    log_message() {
        echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a ${BACKUP_LOG}
    }
    
    # Function to send alerts
    send_alert() {
        local message="$1"
        local severity="$2"
        
        # Send to Slack
        curl -X POST -H 'Content-type: application/json' \
            --data "{\"text\":\"ðŸ”¥ FlexTime DB Backup Alert [${severity}]: ${message}\"}" \
            ${SLACK_WEBHOOK_URL}
            
        # Send to PagerDuty
        if [ "${severity}" = "CRITICAL" ]; then
            curl -X POST https://events.pagerduty.com/v2/enqueue \
                -H 'Content-Type: application/json' \
                -d "{
                    \"routing_key\": \"${PAGERDUTY_ROUTING_KEY}\",
                    \"event_action\": \"trigger\",
                    \"payload\": {
                        \"summary\": \"FlexTime Database Backup Failure\",
                        \"severity\": \"critical\",
                        \"source\": \"flextime-backup-system\",
                        \"custom_details\": {
                            \"message\": \"${message}\",
                            \"timestamp\": \"$(date -Iseconds)\"
                        }
                    }
                }"
        fi
    }
    
    # Function to cleanup old backups
    cleanup_old_backups() {
        log_message "Starting cleanup of old backups (retention: ${BACKUP_RETENTION_DAYS} days)"
        
        # Clean local backups
        find ${BACKUP_DIR} -name "flextime_backup_*.sql*" -mtime +${BACKUP_RETENTION_DAYS} -delete
        
        # Clean S3 backups
        aws s3 ls s3://${S3_BUCKET}/database-backups/ | while read -r line; do
            backup_date=$(echo $line | awk '{print $1}')
            backup_file=$(echo $line | awk '{print $4}')
            
            if [ -n "$backup_date" ]; then
                backup_epoch=$(date -d "$backup_date" +%s)
                cutoff_epoch=$(date -d "${BACKUP_RETENTION_DAYS} days ago" +%s)
                
                if [ $backup_epoch -lt $cutoff_epoch ]; then
                    log_message "Deleting old backup: $backup_file"
                    aws s3 rm s3://${S3_BUCKET}/database-backups/$backup_file
                fi
            fi
        done
        
        log_message "Cleanup completed"
    }
    
    # Main backup function
    perform_backup() {
        log_message "Starting FlexTime database backup"
        
        # Check database connectivity
        if ! pg_isready; then
            send_alert "Database is not ready for backup" "CRITICAL"
            exit 1
        fi
        
        # Create compressed backup
        log_message "Creating compressed backup: ${BACKUP_FILE}"
        
        if pg_dump --verbose --format=custom --compress=9 \
            --exclude-table='logs_*' \
            --exclude-table='temp_*' \
            --exclude-table='sessions' \
            ${PGDATABASE} > ${BACKUP_DIR}/${BACKUP_FILE} 2>>${BACKUP_LOG}; then
            
            log_message "Database backup completed successfully"
            
            # Compress further with gzip
            gzip ${BACKUP_DIR}/${BACKUP_FILE}
            BACKUP_FILE="${BACKUP_FILE}.gz"
            
            # Upload to S3
            log_message "Uploading backup to S3: s3://${S3_BUCKET}/database-backups/${BACKUP_FILE}"
            
            if aws s3 cp ${BACKUP_DIR}/${BACKUP_FILE} s3://${S3_BUCKET}/database-backups/${BACKUP_FILE} \
                --storage-class STANDARD_IA \
                --metadata "backup-date=$(date -Iseconds),database=${PGDATABASE},version=$(pg_config --version)"; then
                
                log_message "Backup uploaded to S3 successfully"
                
                # Verify backup integrity
                log_message "Verifying backup integrity"
                if aws s3api head-object --bucket ${S3_BUCKET} --key database-backups/${BACKUP_FILE}; then
                    log_message "Backup verification successful"
                    
                    # Update backup metrics
                    BACKUP_SIZE=$(stat -c%s ${BACKUP_DIR}/${BACKUP_FILE})
                    curl -X POST http://prometheus-pushgateway:9091/metrics/job/postgres-backup \
                        -d "postgres_backup_size_bytes{database=\"${PGDATABASE}\"} ${BACKUP_SIZE}"
                    curl -X POST http://prometheus-pushgateway:9091/metrics/job/postgres-backup \
                        -d "postgres_backup_success{database=\"${PGDATABASE}\"} 1"
                        
                else
                    send_alert "Backup verification failed" "WARNING"
                fi
                
            else
                send_alert "Failed to upload backup to S3" "CRITICAL"
                exit 1
            fi
            
        else
            send_alert "Database backup failed" "CRITICAL"
            exit 1
        fi
        
        # Cleanup old backups
        cleanup_old_backups
        
        # Remove local backup file
        rm -f ${BACKUP_DIR}/${BACKUP_FILE}
        
        log_message "Backup process completed successfully"
    }
    
    # Execute backup
    perform_backup

  monitoring_queries.sql: |
    -- Database Performance Monitoring Queries
    
    -- Active connections by state
    CREATE OR REPLACE VIEW active_connections AS
    SELECT 
        state,
        count(*) as connection_count,
        max(now() - state_change) as max_duration
    FROM pg_stat_activity 
    WHERE state IS NOT NULL
    GROUP BY state;
    
    -- Top queries by execution time
    CREATE OR REPLACE VIEW slow_queries AS
    SELECT 
        query,
        calls,
        total_exec_time,
        mean_exec_time,
        stddev_exec_time,
        rows,
        100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
    FROM pg_stat_statements 
    ORDER BY total_exec_time DESC
    LIMIT 20;
    
    -- Table sizes and bloat estimation
    CREATE OR REPLACE VIEW table_sizes AS
    SELECT 
        schemaname,
        tablename,
        pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
        pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) as table_size,
        pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) - pg_relation_size(schemaname||'.'||tablename)) as index_size
    FROM pg_tables 
    WHERE schemaname = 'public'
    ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;
    
    -- Index usage statistics
    CREATE OR REPLACE VIEW index_usage AS
    SELECT 
        schemaname,
        tablename,
        indexname,
        idx_tup_read,
        idx_tup_fetch,
        idx_scan,
        pg_size_pretty(pg_relation_size(indexrelid)) as index_size
    FROM pg_stat_user_indexes 
    ORDER BY idx_scan DESC;
    
    -- Lock monitoring
    CREATE OR REPLACE VIEW current_locks AS
    SELECT 
        pg_class.relname,
        pg_locks.locktype,
        pg_locks.mode,
        pg_locks.granted,
        pg_stat_activity.pid,
        pg_stat_activity.query,
        pg_stat_activity.state
    FROM pg_locks
    JOIN pg_class ON pg_locks.relation = pg_class.oid
    JOIN pg_stat_activity ON pg_locks.pid = pg_stat_activity.pid
    WHERE NOT pg_locks.granted
    ORDER BY pg_class.relname;
    
    -- Database statistics
    CREATE OR REPLACE VIEW db_stats AS
    SELECT 
        datname,
        numbackends,
        xact_commit,
        xact_rollback,
        blks_read,
        blks_hit,
        tup_returned,
        tup_fetched,
        tup_inserted,
        tup_updated,
        tup_deleted,
        conflicts,
        temp_files,
        temp_bytes,
        deadlocks
    FROM pg_stat_database
    WHERE datname = current_database();

  optimize_queries.sql: |
    -- Database Optimization Queries
    
    -- Analyze all tables
    DO $$
    DECLARE
        table_record RECORD;
    BEGIN
        FOR table_record IN
            SELECT schemaname, tablename 
            FROM pg_tables 
            WHERE schemaname = 'public'
        LOOP
            EXECUTE 'ANALYZE ' || quote_ident(table_record.schemaname) || '.' || quote_ident(table_record.tablename);
            RAISE NOTICE 'Analyzed table: %.%', table_record.schemaname, table_record.tablename;
        END LOOP;
    END $$;
    
    -- Reindex heavily used indexes
    DO $$
    DECLARE
        index_record RECORD;
    BEGIN
        FOR index_record IN
            SELECT schemaname, indexname, tablename
            FROM pg_stat_user_indexes 
            WHERE idx_scan > 1000
            ORDER BY idx_scan DESC
            LIMIT 10
        LOOP
            EXECUTE 'REINDEX INDEX CONCURRENTLY ' || quote_ident(index_record.schemaname) || '.' || quote_ident(index_record.indexname);
            RAISE NOTICE 'Reindexed: %.%', index_record.schemaname, index_record.indexname;
        END LOOP;
    END $$;
    
    -- Update table statistics for query planner
    UPDATE pg_class SET relpages = relpages WHERE relkind = 'r';
    
    -- Clean up old pg_stat_statements data
    SELECT pg_stat_statements_reset();

---
# PostgreSQL Connection Pooler (PgBouncer)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pgbouncer
  namespace: flextime-production
  labels:
    app: pgbouncer
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: pgbouncer
  template:
    metadata:
      labels:
        app: pgbouncer
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9127"
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1001
        fsGroup: 1001
      containers:
      - name: pgbouncer
        image: pgbouncer/pgbouncer:1.21.0
        env:
        - name: DATABASES_HOST
          valueFrom:
            secretKeyRef:
              name: postgres-connection
              key: host
        - name: DATABASES_PORT
          value: "5432"
        - name: DATABASES_USER
          valueFrom:
            secretKeyRef:
              name: postgres-connection
              key: username
        - name: DATABASES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-connection
              key: password
        - name: DATABASES_DBNAME
          valueFrom:
            secretKeyRef:
              name: postgres-connection
              key: database
        - name: POOL_MODE
          value: "transaction"
        - name: SERVER_RESET_QUERY
          value: "DISCARD ALL"
        - name: MAX_CLIENT_CONN
          value: "1000"
        - name: DEFAULT_POOL_SIZE
          value: "20"
        - name: MIN_POOL_SIZE
          value: "5"
        - name: RESERVE_POOL_SIZE
          value: "3"
        - name: RESERVE_POOL_TIMEOUT
          value: "5"
        - name: MAX_DB_CONNECTIONS
          value: "50"
        - name: MAX_USER_CONNECTIONS
          value: "50"
        - name: SERVER_ROUND_ROBIN
          value: "1"
        - name: IGNORE_STARTUP_PARAMETERS
          value: "extra_float_digits"
        - name: SERVER_IDLE_TIMEOUT
          value: "600"
        - name: SERVER_CONNECT_TIMEOUT
          value: "15"
        - name: SERVER_LOGIN_RETRY
          value: "15"
        - name: CLIENT_LOGIN_TIMEOUT
          value: "60"
        - name: AUTODB_IDLE_TIMEOUT
          value: "3600"
        - name: DNS_MAX_TTL
          value: "15"
        - name: DNS_NXDOMAIN_TTL
          value: "15"
        - name: ADMIN_USERS
          value: "postgres,admin"
        - name: STATS_USERS
          value: "stats,admin"
        ports:
        - containerPort: 5432
          name: postgres
          protocol: TCP
        - containerPort: 6432
          name: admin
          protocol: TCP
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        livenessProbe:
          tcpSocket:
            port: 5432
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          tcpSocket:
            port: 5432
          initialDelaySeconds: 10
          periodSeconds: 10
        securityContext:
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          runAsUser: 1001
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
      - name: pgbouncer-exporter
        image: prometheuscommunity/pgbouncer-exporter:v0.7.0
        env:
        - name: PGBOUNCER_EXPORTER_HOST
          value: "127.0.0.1"
        - name: PGBOUNCER_EXPORTER_PORT
          value: "6432"
        - name: PGBOUNCER_EXPORTER_USERNAME
          value: "stats"
        - name: PGBOUNCER_EXPORTER_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-connection
              key: password
        ports:
        - containerPort: 9127
          name: metrics
          protocol: TCP
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
          limits:
            cpu: 200m
            memory: 256Mi

---
# PgBouncer Service
apiVersion: v1
kind: Service
metadata:
  name: pgbouncer
  namespace: flextime-production
  labels:
    app: pgbouncer
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9127"
spec:
  type: ClusterIP
  ports:
  - port: 5432
    targetPort: 5432
    name: postgres
    protocol: TCP
  - port: 9127
    targetPort: 9127
    name: metrics
    protocol: TCP
  selector:
    app: pgbouncer

---
# Database Backup CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-backup
  namespace: flextime-production
  labels:
    app: postgres-backup
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  timeZone: "UTC"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 7
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: postgres-backup
        spec:
          restartPolicy: OnFailure
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
            fsGroup: 1001
          containers:
          - name: postgres-backup
            image: postgres:15-alpine
            command:
            - /bin/bash
            - /scripts/backup_config.sh
            env:
            - name: POSTGRES_HOST
              valueFrom:
                secretKeyRef:
                  name: postgres-connection
                  key: host
            - name: POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  name: postgres-connection
                  key: username
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-connection
                  key: password
            - name: POSTGRES_DB
              valueFrom:
                secretKeyRef:
                  name: postgres-connection
                  key: database
            - name: S3_BACKUP_BUCKET
              valueFrom:
                configMapKeyRef:
                  name: backup-config
                  key: s3_bucket
            - name: BACKUP_RETENTION_DAYS
              value: "30"
            - name: SLACK_WEBHOOK_URL
              valueFrom:
                secretKeyRef:
                  name: notification-secrets
                  key: slack_webhook_url
            - name: PAGERDUTY_ROUTING_KEY
              valueFrom:
                secretKeyRef:
                  name: notification-secrets
                  key: pagerduty_routing_key
            resources:
              requests:
                cpu: 200m
                memory: 512Mi
              limits:
                cpu: 1000m
                memory: 2Gi
            volumeMounts:
            - name: backup-scripts
              mountPath: /scripts
            - name: backup-storage
              mountPath: /tmp/backups
            securityContext:
              allowPrivilegeEscalation: false
              runAsNonRoot: true
              runAsUser: 1001
              readOnlyRootFilesystem: false
              capabilities:
                drop:
                - ALL
          volumes:
          - name: backup-scripts
            configMap:
              name: postgres-optimization-config
              defaultMode: 0755
          - name: backup-storage
            emptyDir:
              sizeLimit: 10Gi

---
# Database Monitoring CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-monitoring
  namespace: flextime-production
  labels:
    app: postgres-monitoring
spec:
  schedule: "*/5 * * * *"  # Every 5 minutes
  timeZone: "UTC"
  concurrencyPolicy: Allow
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: postgres-monitoring
        spec:
          restartPolicy: OnFailure
          containers:
          - name: postgres-monitor
            image: postgres:15-alpine
            command:
            - /bin/bash
            - -c
            - |
              # Execute monitoring queries and export metrics
              psql -f /scripts/monitoring_queries.sql
              
              # Export connection metrics
              ACTIVE_CONNECTIONS=$(psql -t -c "SELECT count(*) FROM pg_stat_activity WHERE state = 'active';")
              curl -X POST http://prometheus-pushgateway:9091/metrics/job/postgres-monitoring \
                -d "postgres_active_connections{database=\"${POSTGRES_DB}\"} ${ACTIVE_CONNECTIONS}"
              
              # Export slow query count
              SLOW_QUERIES=$(psql -t -c "SELECT count(*) FROM pg_stat_statements WHERE mean_exec_time > 1000;")
              curl -X POST http://prometheus-pushgateway:9091/metrics/job/postgres-monitoring \
                -d "postgres_slow_queries{database=\"${POSTGRES_DB}\"} ${SLOW_QUERIES}"
                
              # Export database size
              DB_SIZE=$(psql -t -c "SELECT pg_database_size(current_database());")
              curl -X POST http://prometheus-pushgateway:9091/metrics/job/postgres-monitoring \
                -d "postgres_database_size_bytes{database=\"${POSTGRES_DB}\"} ${DB_SIZE}"
            env:
            - name: PGHOST
              valueFrom:
                secretKeyRef:
                  name: postgres-connection
                  key: host
            - name: PGUSER
              valueFrom:
                secretKeyRef:
                  name: postgres-connection
                  key: username
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-connection
                  key: password
            - name: PGDATABASE
              valueFrom:
                secretKeyRef:
                  name: postgres-connection
                  key: database
            - name: POSTGRES_DB
              valueFrom:
                secretKeyRef:
                  name: postgres-connection
                  key: database
            resources:
              requests:
                cpu: 50m
                memory: 64Mi
              limits:
                cpu: 200m
                memory: 256Mi
            volumeMounts:
            - name: monitoring-scripts
              mountPath: /scripts
          volumes:
          - name: monitoring-scripts
            configMap:
              name: postgres-optimization-config

---
# Database Optimization CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: postgres-optimization
  namespace: flextime-production
  labels:
    app: postgres-optimization
spec:
  schedule: "0 3 * * 0"  # Weekly on Sunday at 3 AM
  timeZone: "UTC"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 4
  failedJobsHistoryLimit: 2
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: postgres-optimization
        spec:
          restartPolicy: OnFailure
          containers:
          - name: postgres-optimizer
            image: postgres:15-alpine
            command:
            - /bin/bash
            - -c
            - |
              echo "Starting weekly database optimization..."
              
              # Run optimization queries
              psql -f /scripts/optimize_queries.sql
              
              # Vacuum analyze large tables
              psql -c "VACUUM ANALYZE;"
              
              echo "Database optimization completed successfully"
            env:
            - name: PGHOST
              valueFrom:
                secretKeyRef:
                  name: postgres-connection
                  key: host
            - name: PGUSER
              valueFrom:
                secretKeyRef:
                  name: postgres-connection
                  key: username
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-connection
                  key: password
            - name: PGDATABASE
              valueFrom:
                secretKeyRef:
                  name: postgres-connection
                  key: database
            resources:
              requests:
                cpu: 200m
                memory: 256Mi
              limits:
                cpu: 1000m
                memory: 1Gi
            volumeMounts:
            - name: optimization-scripts
              mountPath: /scripts
          volumes:
          - name: optimization-scripts
            configMap:
              name: postgres-optimization-config